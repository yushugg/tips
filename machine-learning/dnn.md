## 评测好坏

1. 速度效率
2. 内存需求
3. 效果如何

## 梯度下降优化算法

学习率η决定达到最小值或者局部最小值过程中所采用的步长的大小

### 梯度下降法的变形形式

#### 批梯度下降(batch gradient descent)

在**整个训练数据集**上计算loss关于参数θ的梯度

    θ=θ−η⋅∇θJ(θ)

1. 速度慢
2. 需要将全部的数据集倒入内存中，对内存大小要求较高；每次更新一个参数会对相似的样本计算梯度，计算过程会有冗余
3. 无法在线更新模型，即运行时无法增加新的样本
4. 对于凸函数，可以收敛到全局最小；对于非凸函数，收敛到一个局部最小值

#### 随机梯度下降(stochastic gradient descent, SGD)

根据每一条训练样本x(i)和标签y(i)更新参数

    θ=θ−η⋅∇θJ(θ;x(i);y(i))

1. 速度快
2. 内存需求小；每次更新只执行一次，消除了计算冗余
3. 可以用于在线学习
4. SGD以高方差频繁地更新，导致目标函数出现剧烈波动；如果结果是局部最优：一方面，SGD可能跳到新的或潜在的更好的局部最优；另一方面，收敛过程会变得复杂，然而已经证明当**缓慢减小学习率**，SGD和批梯度下降具有相同的收敛行为，对于非凸和凸分别收敛到局部最小和全局最小；如果步长依然较大，就会产生振荡，在极小点附近来回跳动；SGD在**凸函数下证明是按期望严格收敛的**

#### 小批量梯度下降(也成为SGD, Minibatch SGD)

每次更新使用n个小批量训练样本，大体上和SGD类似

    θ=θ−η⋅∇θJ(θ;x(i:i+n);y(i:i+n))

1. 减少参数更新的方差，得到更加稳定的收敛结果
2. 可以使用深度学习库中矩阵优化的方法，高效求解梯度

### 梯度下降优化算法

#### 动量法

1. SGD很难通过沟壑，通常是局部最优点，这种情况下，SGD摇摆地通过陡谷的斜坡，同时沿着底部到局部最优点的路径上只是缓慢的前进
2. 动量法可以帮助SGD在相关方向上加速并抑制摇摆的方法，动量法将历史步长的更新向量的一个分量增加到当前的更新向量中

        v(t)=γv(t−1)+η∇θJ(θ)
        θ=θ−v(t)

    对于在梯度点具有相同的方向的维度，动量项增大，在梯度点处改变方向的维度，动量项减小；因此可以更快的收敛，同时减少摇摆

#### Nesterov加速梯度下降法

1. 动量法盲目的沿着斜率方向，Nesterov能给动量项增加预知方向能力，通过计算关于参数未来的近似位置的梯度，而不是关于当前的参数θ的梯度，我们可以高效的求解，对动量法的一个修正

        v(t)=γv(t−1)+η∇θJ(θ−γv(t−1))
        θ=θ−v(t)

#### Adagrad

1. 让学习率适应参数，对于出现次数少的特征，采用更大的学习率，对于出现次数多的特征，采用较小的学习率，因此**特别适合处理稀疏数据**
2. 令g(t,i)为在t时刻目标函数关于参数θi的梯度

        g(t,i)=∇θJ(θi)

    在t时刻，对每个参数θi的更新过程为：

        θ(t+1,i)=θ(t,i)−η⋅g(t,i)

    Adagrad修正了对每一个参数θi的学习率：

        θ(t+1)=θ(t)-η/sqrt(G(t)+ϵ)⋅g(t,i)

3. 优点：无需手动调整学习率；缺点：分母中累加梯度的平方，平方都为正，训练过程中累加和会持续增长，导致学习率变小，学习率无限小时，Adagrad无法取得额外的信息

#### Adadelta

可以处理Adagrad学习速率单调递减的问题，不是计算所有的梯度平方，Adadelta将计算计算历史梯度的窗口大小限制为一个固定值w；在Adadelta中，无需存储先前的w个平方梯度，而是将梯度的平方递归地表示成**所有历史梯度平方的均值**，在t时刻的均值E\[g^2](t)只取决于先前的均值和当前的梯度（分量γ类似于动量项）

    E[g^2](t)=γE[g^2](t−1)+(1−γ)g^2(t)

使用Adadelta算法，我们甚至都无需设置默认的学习率，因为更新规则中已经移除了学习率

#### RMSprop

MSprop是先前我们得到的Adadelta的第一个更新向量的特例，同样，RMSprop将学习率分解成一个平方梯度的指数衰减的平均

#### Adam(自适应矩估计)

Adam对每一个参数都计算自适应的学习率。除了像Adadelta和RMSprop一样存储一个指数衰减的历史平方梯度的平均v(t)，Adam同时还保存一个历史梯度的指数衰减均值m(t)，类似于动量

    m(t)=β1m(t−1)+(1−β1)g(t)
    v(t)=β2v(t−1)+(1−β2)g^2(t)

m(t)和v(t)分别是对梯度的一阶矩（均值）和二阶矩（非确定的方差）的估计，通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差

Adam相比RMSprop，增加了**偏差校正和动量**

### 批量归一化(Batch normalization)

1. 为了便于学习，我们通常用0均值和单位方差初始化我们的参数的初始值来归一化。随着不断训练，参数得到不同的程度的更新，我们失去了这种归一化，随着网络变得越来越深，这种现象会降低训练速度，且放大参数变化。
2. 批量归一化在每次小批量数据反向传播之后重新对参数进行0均值单位方差标准化。通过将模型架构的一部分归一化，我们能够使用更高的学习率，更少关注初始化参数。
3. 批量归一化还充当正则化的作用，减少（有时甚至消除）Dropout的必要性。

## 激活函数

### sigmoid

1. g(z) = 1 / (1 + e^-z)
2. 值域在0-1之间，当z非常大时，g(z)趋近于1；当z非常小时，g(z)趋近于0，适合于分类，可表示概率

## NN

隐藏层

    a1 = g(w10x0 + w11x1 + w12x2 + b1)
    a2 = g(w20x0 + w21x1 + w22x2 + b2)
    ...
    an = g(wn0x0 + wn1x1 + wn2x2 + bn)

## DNN

## CNN

1. conv + relu + pooling + fc

### conv

1. 权值共享：一个filter的w是确定的，用此一个filter去卷积图像的所有部分，同一个filter在图像不同位置上进行卷积时的w是不变的；因此卷积层的参数数量为filter size * filter number
2. depth: filter的个数，stride: 移动步长，zero-padding: 在外围边缘补充0
3. 参数：input, filter, strides, padding

    input: [batch, in_height, in_width, in_channels]
    filter: [filter_height, filter_width, in_channels, out_channels]
    strides: [1, 1, 1, 1]
    padding: padding的方法，"SAME"或者"VALID"
    output: 

### pooling

1. 作用：依靠局部不变性，下采样，降低维度，防止过拟合

## RNN

## LSTM

## GRU
